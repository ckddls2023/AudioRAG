exp_name: 'train'
seed: 20
train_jsons: [
  'data/json_files/AudioSet/train.json',
  'data/json_files/Clotho/train.json',
]
val_jsons: [
  "data/json_files/AudioSet/val.json",
]
blacklist: ""
# dataset == 'AudioCaps': sampling_rate = 32000
# dataset == 'Clotho': sampling_rate = 44100
# 48000 for CLAP
# 32000 for HTSAT
audio_args:
  sr: 48000
  n_fft: 1024
  hop_length: 320
  f_min: 50
  f_max: 14000
  n_mels: 64
  max_length: 10
  mono: True

data_args:
  dataset: "Clotho"
  global_batch_size: 256
  batch_size: 8
  num_workers: 16

model_args:
  name: "CLAP2LLAMA"
  freeze_am: True
  freeze_lm: True
  checkpoint_path: "./pretrained_models/pretrained_LGTM/"
  encoder:
    model_name: "CLAPAudioEncoder"
    pretrained: True
    freeze: True
    spec_augment: True
    select_feature: "fine_grained_embedding"
    sequence_length: 1024
    hidden_size: 768
    window_size: 8 # (32,32) = [B,32,H], (16,16) = [B,64,H], (8,8) = [B,128,H] (4,4) = [B,256,H]
    step_size: 8
  align:
    model_name: "LGTM"
  peft_config:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"
    modules_to_save: ['lm_head', 'embed_tokens']
    target_modules: ["q_proj", "v_proj"]

# lr from wavcaps, linear scale lr
optim_args:
  lr: 2e-4
  betas: [0.9, 0.99]
  eps: 1e-8
  momentum: 0.9
  gamma: 0.1
  weight_decay: 0.01

training:
  warmup_epochs: 2
  epochs: 10
  clip_grad: 2
  output_path: "./finetuned_models/audio_caption_LGTM/"
  eval: True
  validate: False

index_args:
  index_path: "./data/index/train_audio_512_audiopath.json"
  top_k: 2
