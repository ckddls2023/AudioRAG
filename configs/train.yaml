exp_name: 'train'
seed: 20

train_jsons: [
  'data/json_files/BBC_Sound_Effects/bbc_final.json',
  'data/json_files/FreeSound/fsd_final.json',
  'data/json_files/SoundBible/sb_final.json',
  "data/json_files/AudioSet_SL/as_final.json",
  #"data/json_files/Clotho/clotho_final.json"
]
# train_jsons: [
#   'data/json_files/AudioSet/train.json',
#   'data/json_files/Clotho/train.json',
# ]
val_jsons: [
  "data/json_files/AudioSet/val.json",
]
blacklist: ""
# dataset == 'AudioCaps': sampling_rate = 32000
# dataset == 'Clotho': sampling_rate = 44100
# 48000 for CLAP
# 32000 for HTSAT
audio_args:
  sr: 48000
  n_fft: 1024
  hop_length: 320
  f_min: 50
  f_max: 14000
  n_mels: 64
  max_length: 10
  mono: True

data_args:
  dataset: "Clotho"
  global_batch_size: 256
  batch_size: 4
  num_workers: 16

model_args:
  name: "CLAP2LLAMA"
  freeze_am: True
  freeze_lm: False
  checkpoint_path: "./pretrained_models/pretrained_MLP/"
  encoder:
    model_name: "CLAPAudioEncoder"
    pretrained: True
    freeze: True
    spec_augment: True
    select_feature: "fine_grained_embedding"
    sequence_length: 1024
    hidden_size: 768
    window_size: 32 # (32,32) = [B,32,H], (16,16) = [B,64,H], (8,8) = [B,128,H] (4,4) = [B,256,H]
    step_size: 16
  align:
    model_name: "MLP"
  peft_config:
    type: "LORA"
    r: 8
    lora_alpha: 16
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"
    modules_to_save: ['lm_head', 'embed_tokens']
    target_modules: ["q_proj", "v_proj"]
  # peft_config:
  #   type: "IA3"
  #   peft_type: "IA3"
  #   task_type: "CAUSAL_LM"
  #   target_modules: ["q_proj", "v_proj", "down_proj"]
  #   feedforward_modules: ["down_proj"]

# lr from wavcaps, linear scale lr
optim_args:
  lr: 1e-4
  betas: [0.9, 0.99]
  eps: 1e-8
  momentum: 0.9
  gamma: 0.1
  weight_decay: 1e-6

training:
  warmup_epochs: 2
  epochs: 10
  clip_grad: 2
  output_path: "./finetuned_models/finetuned_MLP/"
  eval: False
  validate: False
  
index_args:
  index_path: ""
  # index_path: "./data/index/final_index_big_kb/frame2frame_train.json"
  top_k: 2
